# ğŸï¸ F1 CLIP Analysis

Comparing Pre-CLIP and CLIP multimodal embeddings on a Formula 1 dataset.

## ğŸ“‹ Overview

This project analyzes image-text similarity using two approaches:

- **Pre-CLIP** : Separate ResNet50 (images) + sentence-transformers (text)
- **CLIP** : Unified multimodal model

Dataset: 20 F1 images across 4 categories with manually written captions.

---

## ğŸ› ï¸ Technologies

- **PyTorch 2.1.0** (CUDA 11.8)
- **Transformers** (HuggingFace CLIP)
- **sentence-transformers** (distiluse-base-multilingual)
- **torchvision** (ResNet50)
- **Docker** (containerized environment)
- **Python 3.10**

---

## ğŸ“ Project Structure

```
f1-clip-analysis/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ dataset/                      # âš ï¸ NOT INCLUDED (see below)
â”‚   â”œâ”€â”€ 1_drivers_emotions/
â”‚   â”œâ”€â”€ 2_pit_stops/
â”‚   â”œâ”€â”€ 3_cars_tracks_moments/
â”‚   â”œâ”€â”€ 4_strategy_data/
â”‚   â””â”€â”€ captions.json
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb           # Analysis notebook
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ clip/
â”‚   â”‚   â”œâ”€â”€ image_embeddings.npy
â”‚   â”‚   â”œâ”€â”€ image_metadata.csv
â”‚   â”‚   â”œâ”€â”€ similarity_matrix.csv
â”‚   â”‚   â”œâ”€â”€ text_embeddings.npy
â”‚   â”‚   â””â”€â”€ text_metadata.csv
â”‚   â””â”€â”€ preclip/
â”‚       â”œâ”€â”€ image_embeddings.npy
â”‚       â”œâ”€â”€ image_metadata.csv
â”‚       â”œâ”€â”€ similarity_matrix.csv
â”‚       â”œâ”€â”€ text_embeddings.npy
â”‚       â””â”€â”€ text_metadata.csv
â””â”€â”€ src/
    â”œâ”€â”€ utils.py                 # Common utilities (dataset, similarity, saving)
    â”œâ”€â”€ clip/
    â”‚   â”œâ”€â”€ clip_models.py       # CLIP model
    â”‚   â””â”€â”€ run_clip.py          # Task 3: CLIP analysis
    â””â”€â”€ preclip/
        â”œâ”€â”€ preclip_models.py    # ResNet + sentence-transformer
        â”œâ”€â”€ preclip_utils.py     # Pre-CLIP specific (PCA projection)
        â””â”€â”€ run_preclip.py       # Task 2: Pre-CLIP analysis
```

---

## âš ï¸ Dataset Warning

**The dataset is NOT included in this repository** due to size/licensing constraints.
If needed, contact me through my links of my bio.

The dataset structure required:

```
dataset/
â”œâ”€â”€ 1_drivers_emotions/driver_01.jpg â†’ driver_05.jpg
â”œâ”€â”€ 2_pit_stops/pitstop_01.jpg â†’ pitstop_05.jpg
â”œâ”€â”€ 3_cars_tracks_moments/car_01.jpg â†’ car_05.jpg
â”œâ”€â”€ 4_strategy_data/strategy_01.jpg â†’ strategy_05.jpg
â””â”€â”€ captions.json
```

---

## ğŸš€ Quick Start

**Prerequisites:**

- Docker with GPU support ([nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html))
- NVIDIA GPU with CUDA support

**Build & Run:**

```bash
# Build Docker image
make build

# Run Pre-CLIP analysis (Task 2)
make run-preclip

# Run CLIP analysis (Task 3)
make run-clip

# Compare results
make compare

# Interactive shell
make shell
```

---

## ğŸ“Š Results

Results are saved in `results/` with:

- Similarity matrices (CSV)
- Visualizations (heatmaps, plots)
- Embedding files (.npz)

## ğŸ“ License

Apache License 2.0

## Author

VÃ­ctor Vega Sobral - UIE 2024/2025
